You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_685 x:0.49 y:0.5 width:0.99 height:1.0 color:darkslategray, bottle_709 x:0.51 y:0.68 width:0.18 height:0.64 color:chocolate, bottle_710 x:0.29 y:0.71 width:0.11 height:0.57 color:gray, bottle_711 x:0.77 y:0.76 width:0.16 height:0.48 color:darkslategray]
'task description':
[Q] Which bottle is metal?
'response':
---
{
  "id": "chatcmpl-9FYhpvqV4u7vlEkEGmQtWzftURtTn",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is metal, bottle_709, bottle_710, or bottle_711? Output only the bottle ID';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713495357,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 32,
    "prompt_tokens": 3066,
    "total_tokens": 3098
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the 'id' (label) of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple with id apple_20 on the table, you could respond 'apple_20')
    Here is a list of objects which have been given ids in the scene, as well as their xy coordinates, width, height, and average color. Mostly, only the ids will be useful: [person_685 x:0.54 y:0.5 width:0.91 height:1.0 color:darkslategray, bottle_709 x:0.53 y:0.68 width:0.19 height:0.64 color:darkslategray, bottle_710 x:0.32 y:0.71 width:0.11 height:0.57 color:gray, bottle_711 x:0.79 y:0.76 width:0.16 height:0.48 color:darkslategray]
    Now, answer the following question from the drone:  Which bottle is metal, bottle_709, bottle_710, or bottle_711? Output only the bottle ID

Response: bottle_710
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_685 x:0.51 y:0.5 width:0.97 height:1.0 color:darkslategray, bottle_709 x:0.53 y:0.68 width:0.19 height:0.64 color:midnightblue, bottle_710 x:0.31 y:0.71 width:0.1 height:0.57 color:gray, bottle_711 x:0.79 y:0.76 width:0.16 height:0.48 color:darkslategray]
'task description':
[A] Which bottle has the darkest color?
'response':
---
{
  "id": "chatcmpl-9FYiCl8htcg1RjAa16TeK0nAONgeo",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle has the darkest color, bottle_709, bottle_710, or bottle_711? Output only the label of the bottle';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713495380,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 36,
    "prompt_tokens": 3068,
    "total_tokens": 3104
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the 'id' (label) of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple with id apple_20 on the table, you could respond 'apple_20')
    Here is a list of objects which have been given ids in the scene, as well as their xy coordinates, width, height, and average color. Mostly, only the ids will be useful: [person_685 x:0.54 y:0.5 width:0.9 height:1.0 color:darkslategray, bottle_709 x:0.53 y:0.68 width:0.19 height:0.64 color:darkslategray, bottle_710 x:0.32 y:0.71 width:0.11 height:0.57 color:gray, bottle_711 x:0.79 y:0.76 width:0.16 height:0.48 color:darkslategray]
    Now, answer the following question from the drone:  Which bottle has the darkest color, bottle_709, bottle_710, or bottle_711? Output only the label of the bottle

Response: bottle_709
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_719 x:0.75 y:0.7 width:0.14 height:0.59 color:darkgray, bottle_710 x:0.29 y:0.68 width:0.19 height:0.65 color:peru, bottle_720 x:0.51 y:0.75 width:0.14 height:0.5 color:darkslategray, refrigerator_722 x:0.56 y:0.5 width:0.84 height:1.0 color:darkslategray, person_723 x:0.37 y:0.5 width:0.7 height:1.0 color:darkslategray, cup_724 x:0.97 y:0.91 width:0.05 height:0.18 color:dimgray]
'task description':
[A] Ask LLaVA which bottle is metal, log the object id, then orient towards it
'response':
---
{
  "id": "chatcmpl-9FYjA1N4CtGZZIdi5wEVAdkCW1w7E",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is metal, bottle_719, bottle_710, or bottle_720? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713495440,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 35,
    "prompt_tokens": 3138,
    "total_tokens": 3173
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the 'id' (label) of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple with id apple_20 on the table, you could respond 'apple_20')
    Here is a list of objects which have been given ids in the scene, as well as their xy coordinates, width, height, and average color. Mostly, only the ids will be useful: [bottle_719 x:0.76 y:0.7 width:0.16 height:0.59 color:rosybrown, bottle_710 x:0.29 y:0.68 width:0.19 height:0.65 color:chocolate, bottle_720 x:0.51 y:0.75 width:0.14 height:0.5 color:darkslategray, person_723 x:0.38 y:0.5 width:0.69 height:1.0 color:darkslategray, cup_724 x:0.97 y:0.91 width:0.05 height:0.18 color:gray]
    Now, answer the following question from the drone:  Which bottle is metal, bottle_719, bottle_710, or bottle_720? Output only the object id

Response: bottle_719
----------------------------------
