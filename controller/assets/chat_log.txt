You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.66 y:0.53 width:0.22 height:0.89 color:peru, person_592 x:0.52 y:0.49 width:0.89 height:0.99 color:darkslategray]
'task description':
[Q] What's the scene description? Don't use llava
'response':
---
{
  "id": "chatcmpl-9FXwNth49eBKfc2hb1BuOPoKF92IJ",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "l,'bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.66 y:0.53 width:0.22 height:0.89 color:peru, person_592 x:0.52 y:0.49 width:0.89 height:0.99 color:darkslategray'",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492415,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 91,
    "prompt_tokens": 3046,
    "total_tokens": 3137
  }
}
---
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.66 y:0.53 width:0.22 height:0.89 color:peru, person_592 x:0.51 y:0.5 width:0.9 height:1.0 color:darkslategray]
'task description':
[Q] Ask llava for the object ID of the bottle that has the spray bottle
'response':
---
{
  "id": "chatcmpl-9FXx2bjySwSMYZdXbCSDj2W7q1Izg",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the spray bottle, bottle_569 or bottle_570? Output only the object ID';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492456,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 29,
    "prompt_tokens": 3050,
    "total_tokens": 3079
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the spray bottle, bottle_569 or bottle_570? Output only the object ID

Response: bottle_570
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.67 y:0.53 width:0.21 height:0.88 color:peru, person_592 x:0.51 y:0.49 width:0.9 height:0.99 color:darksalmon]
'task description':
[A] Ask llava for the object ID of the darker bottle
'response':
---
{
  "id": "chatcmpl-9FXxTt3eIhbpeXN8YdM5xSjDQozU8",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is darker, bottle_569 or bottle_570? Output only 'bottle_569' or 'bottle_570'';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492483,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 37,
    "prompt_tokens": 3044,
    "total_tokens": 3081
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is darker, bottle_569 or bottle_570? Output only 'bottle_569' or 'bottle_570

Response: bottle_569
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.67 y:0.53 width:0.21 height:0.9 color:indianred, person_592 x:0.44 y:0.49 width:0.72 height:0.99 color:darkslategray, cup_596 x:0.86 y:0.7 width:0.14 height:0.25 color:dimgray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FXyC866tiU9GmLfdeUApRSFevEL7",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492528,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 21,
    "prompt_tokens": 3084,
    "total_tokens": 3105
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_569
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.68 color:darkslategray, bottle_570 x:0.66 y:0.54 width:0.22 height:0.89 color:peru, person_592 x:0.45 y:0.49 width:0.73 height:0.99 color:darksalmon, cup_600 x:0.86 y:0.7 width:0.14 height:0.25 color:dimgray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FXydSqqpzqU5EwwdipcpGy7LfuR7",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492555,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3085,
    "total_tokens": 3109
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_569
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.66 y:0.53 width:0.2 height:0.88 color:peru, person_592 x:0.42 y:0.49 width:0.78 height:0.99 color:darkslategray, cup_600 x:0.86 y:0.7 width:0.14 height:0.25 color:dimgray]
'task description':
[A] Ask llava for the object id of the bottle in the hand of the  man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FXzC3CzDT4yenfE5bNgDGwRp2aJe",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'What is the id of the bottle in the hand of the man?';?_1/bottle{l,_1;o,_1}->False",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492590,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 33,
    "prompt_tokens": 3088,
    "total_tokens": 3121
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  What is the id of the bottle in the hand of the man?

Response: bottle_569
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.43 y:0.62 width:0.17 height:0.69 color:darkslategray, bottle_570 x:0.67 y:0.54 width:0.21 height:0.89 color:peru, person_592 x:0.41 y:0.5 width:0.81 height:1.0 color:tan, cup_600 x:0.86 y:0.7 width:0.14 height:0.25 color:dimgray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FXzVXrAkBsyfjynf3e05q6FsSMFf",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492609,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3083,
    "total_tokens": 3107
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_569
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_569 x:0.46 y:0.81 width:0.15 height:0.38 color:darkslategray, person_592 x:0.56 y:0.5 width:0.72 height:1.0 color:darkslategray, tv_604 x:0.14 y:0.67 width:0.29 height:0.66 color:darkslategray, bottle_607 x:0.81 y:0.51 width:0.23 height:0.79 color:indianred]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FXzyZ8HuAIHWUFP7v74M20Jj0JFM",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492638,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3089,
    "total_tokens": 3113
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_607
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_607 x:0.75 y:0.76 width:0.22 height:0.49 color:black, person_609 x:0.66 y:0.5 width:0.68 height:1.0 color:darkslategray, bottle_611 x:0.45 y:0.54 width:0.15 height:0.59 color:darkslategray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FY0I0jC9hoHliOdKiiMPSMEwSzer",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492658,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3056,
    "total_tokens": 3080
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_611
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_607 x:0.72 y:0.77 width:0.19 height:0.47 color:darkslategray, person_609 x:0.55 y:0.5 width:0.73 height:1.0 color:darkslategray, bottle_611 x:0.4 y:0.8 width:0.15 height:0.39 color:darkslategray, tv_617 x:0.14 y:0.68 width:0.29 height:0.65 color:darkslategray, bowl_619 x:0.92 y:0.96 width:0.09 height:0.08 color:gray]
'task description':
[A] Ask llava for the object id of the bottle the man is looking at, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FY0owW9PTXMcOxA4Lt91hcqpH9Pl",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the person_609 looking at? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492690,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 27,
    "prompt_tokens": 3117,
    "total_tokens": 3144
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the person_609 looking at? Output only the object id

Response: bottle_611
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_607 x:0.72 y:0.78 width:0.19 height:0.45 color:darkslategray, person_609 x:0.55 y:0.5 width:0.71 height:1.0 color:darkslategray, bottle_611 x:0.4 y:0.8 width:0.15 height:0.39 color:darkslategray, umbrella_617 x:0.14 y:0.68 width:0.29 height:0.65 color:darkslategray, bowl_619 x:0.92 y:0.96 width:0.09 height:0.08 color:gray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FY12vrpru57qHAtxG3F7hjvdmQqS",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492704,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3117,
    "total_tokens": 3141
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: bottle_611
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_607 x:0.75 y:0.73 width:0.2 height:0.54 color:darkslategray, bottle_611 x:0.3 y:0.81 width:0.15 height:0.38 color:darkslategray, umbrella_617 x:0.14 y:0.66 width:0.28 height:0.62 color:darkslategray, person_620 x:0.36 y:0.5 width:0.6 height:1.0 color:darkslategray, bowl_619 x:0.91 y:0.96 width:0.11 height:0.08 color:dimgray]
'task description':
[A] Ask llava for the object id of the bottle being held by the man, log it, then orient towards that object
'response':
---
{
  "id": "chatcmpl-9FY1PsJrUicNZooHJDsgvqZI11agQ",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492727,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 3118,
    "total_tokens": 3142
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Which bottle is the man holding? Output only the object id

Response: the man is holding a bottle with the id "bottle_607".
----------------------------------

------------- MINISPEC ERROR -------------
Original Minispec: _1=lr,'Which bottle is the man holding? Output only the object id';l,_1;o,_1
----
Error:Object . is not in sight.

TRACEBACK: ['  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/llm_controller.py", line 178, in execute_task_description\n    self.execute_minispec(result)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/llm_controller.py", line 154, in execute_minispec\n    return interpreter.execute(minispec)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 66, in execute\n    self.execute_function_call(statement)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 94, in execute_function_call\n    self.call_function(statement)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 146, in call_function\n    return interpreter.execute(skill_instance.execute(args))\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 58, in execute\n    result = self.execute_loop(statement)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 78, in execute_loop\n    result = self.execute(program)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 66, in execute\n    self.execute_function_call(statement)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 92, in execute_function_call\n    self.env[var.strip()] = self.call_function(func)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 141, in call_function\n    return skill_instance.execute(args)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/skillset.py", line 78, in execute\n    return self.skill_callable(*parsed_args)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/vision_skill_wrapper.py", line 71, in object_x\n    raise ValueError(f"Object {object_name} is not in sight.")\n']
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[bottle_607 x:0.76 y:0.73 width:0.19 height:0.54 color:midnightblue, person_630 x:0.42 y:0.5 width:0.62 height:1.0 color:darkslategray, bottle_631 x:0.31 y:0.81 width:0.14 height:0.38 color:darkslategray]
'task description':
[A] Ask llava to log the label of the person, then ask it how far away the person is and log the result
'response':
---
{
  "id": "chatcmpl-9FY3fibAd9J7rA9eNGqcbNTvW9b84",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'What is the label of the person in the frame?';l,_1;_2=lr,'How far away is the person in the frame?';l,_2",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492867,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 40,
    "prompt_tokens": 3059,
    "total_tokens": 3099
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  What is the label of the person in the frame?

Response: person_630
----------------------------------

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  How far away is the person in the frame?

Response: the person in the frame is very close to the camera, as indicated by the bounding box around them.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_630 x:0.57 y:0.5 width:0.86 height:1.0 color:darkslategray, bottle_638 x:0.74 y:0.76 width:0.19 height:0.49 color:darkslategray, bottle_637 x:0.31 y:0.83 width:0.14 height:0.33 color:darkslategray]
'task description':
[A] Ask llava to estimate how far the person is in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY4MDfAXrtO6lx9qz4G7tFbfEsS0",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Estimate the distance to the person in centimeters';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492910,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 19,
    "prompt_tokens": 3053,
    "total_tokens": 3072
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate the distance to the person in centimeters

Response: the image provided does not contain a person. it appears to be a screenshot of a video or a photo of a room with various objects, including a shelf with books and a bottle labeled "bottle_637". the person in the image is not visible.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_630 x:0.55 y:0.5 width:0.91 height:1.0 color:darkslategray, bottle_638 x:0.74 y:0.76 width:0.19 height:0.49 color:darkslategray, bottle_637 x:0.31 y:0.83 width:0.14 height:0.34 color:darkslategray]
'task description':
[A] Ask llava to estimate how far the man is in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY4mX2vb8T5nk3MU67hDOiLq8Y8i",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Estimate the distance to the man in centimeters.';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713492936,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 20,
    "prompt_tokens": 3053,
    "total_tokens": 3073
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate the distance to the man in centimeters.

Response: the man is sitting in front of a shelf with various items on it. he is wearing a black shirt and appears to be looking directly at the camera. the shelf is green and has books and other items on it. the man is also sitting in front of a spray bottle with a red and white label. the distance to the man is approximately 637 centimeters.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_644 x:0.52 y:0.5 width:0.74 height:1.0 color:darkslategray, bottle_655 x:0.49 y:0.72 width:0.13 height:0.56 color:darkslategray, cup_653 x:0.93 y:0.82 width:0.13 height:0.19 color:dimgray]
'task description':
[A] Ask llava to estimate how far the bottle is from the camera and log the result
'response':
---
{
  "id": "chatcmpl-9FY5sJf5LPQDEjaZ9drhlPt1LIZWH",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'How far is the bottle from the camera?';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493004,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 18,
    "prompt_tokens": 3051,
    "total_tokens": 3069
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  How far is the bottle from the camera?

Response: the bottle is approximately 1.5 meters away from the camera.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_644 x:0.57 y:0.5 width:0.8 height:1.0 color:darkslategray, bottle_655 x:0.49 y:0.72 width:0.13 height:0.56 color:darkslategray, bottle_656 x:0.42 y:0.28 width:0.04 height:0.11 color:gray, bottle_657 x:0.45 y:0.27 width:0.03 height:0.09 color:lightslategray]
'task description':
[A] Ask llava to estimate how far the bottle is from the camera and log the result
'response':
---
{
  "id": "chatcmpl-9FY6BJRxpKhYJCfY9ARo9yODIg8kd",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'How far is the bottle from the camera?';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493023,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 18,
    "prompt_tokens": 3079,
    "total_tokens": 3097
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  How far is the bottle from the camera?

Response: the bottle is approximately 655 feet from the camera.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_644 x:0.7 y:0.5 width:0.6 height:1.0 color:darkslategray, bottle_657 x:0.55 y:0.48 width:0.03 height:0.11 color:gray, bottle_674 x:0.51 y:0.48 width:0.03 height:0.11 color:gray]
'task description':
[A] Ask llava to estimate how far the drill press is from the camera in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY76L1RIkCM0p22KkAHRA9FW4kJX",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'How far is the drill press from the camera in centimeters?';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493080,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 22,
    "prompt_tokens": 3051,
    "total_tokens": 3073
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  How far is the drill press from the camera in centimeters?

Response: the drill press is approximately 100 centimeters away from the camera.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_682 x:0.7 y:0.5 width:0.61 height:1.0 color:darkslategray, tv_681 x:0.07 y:0.42 width:0.13 height:0.5 color:darkslategray]
'task description':
[A] Ask llava to estimate how far the drill press is from the camera in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY7KRvVnIl2Q7vHvDcLrxtb2vOX8",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=sl,'Is there a drill press in the frame? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'Estimate how far the drill press is from the camera in centimeters';l,_2;->True}l,'There is no drill press in the area';->False",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493094,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 71,
    "prompt_tokens": 3027,
    "total_tokens": 3098
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Is there a drill press in the frame? Answer only 'yes' or 'no'',

Response: yes
----------------------------------

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate how far the drill press is from the camera in centimeters

Response: the drill press is approximately 100 centimeters away from the camera.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_685 x:0.56 y:0.55 width:0.51 height:0.9 color:darkslategray]
'task description':
[A] Ask llava to estimate how far apart the drill press and the man are in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY8TV4K4K0EIbMcjAEAvvHQ9ex6f",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Estimate how far apart the drill press and the man are in centimeters';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493165,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 2998,
    "total_tokens": 3022
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate how far apart the drill press and the man are in centimeters

Response: the drill press is located to the left of the man, and the man is sitting in a chair. the drill press is approximately 1.5 meters away from the man, which is equivalent to 150 centimeters.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_685 x:0.59 y:0.5 width:0.57 height:1.0 color:darkslategray]
'task description':
[A] Ask llava to estimate how far the drill press is from the camera in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY8ocIRWOJBcc2EzkMADzB3yCiTn",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Estimate how far the drill press is from the camera in centimeters.';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493186,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 24,
    "prompt_tokens": 2997,
    "total_tokens": 3021
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate how far the drill press is from the camera in centimeters.

Response: the drill press is approximately 100 centimeters away from the camera.
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," or "output the label of the object" etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output. Returns LLaVA's output if the substring is found, and False if it isn't found

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more
abbr:r,name:replan,args:[task:str],description:Restart the planning process for the current task, given the new situation and position of the drone.


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
Note that LLaVA can see the labels and bounding boxes in the scene description, so you can ask questions about the objects in the scene description and ask it to output a label.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_685 x:0.59 y:0.5 width:0.72 height:1.0 color:darkslategray]
'task description':
[A] Ask llava to estimate the distance between the man and the drill press in centimeters and log the result
'response':
---
{
  "id": "chatcmpl-9FY9MY0UA6uDnW18qy0wvDr6YT2Gv",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Estimate the distance between the man and the drill press in centimeters';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1713493220,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 23,
    "prompt_tokens": 2997,
    "total_tokens": 3020
  }
}
---

------------- LLAVA REQUEST --------
Question: You are the vision model for a drone that is attempting to accomplish tasks. 
    The planning module of the drone will use you to gather specific information about the visible scene by asking you a question. When the question is asking for a specific piece of information, your answers should be as concise as possible, such as ‘which direction is person_20 facing?’ (Answer should be ‘Right’ or ‘Left’.) However, when the question is open ended and less concrete, such as 'tell me about the scene in front of you', or ‘describe chair_1’, you can answer normally. 
    If you are asked to provide an object, use the label of the object whenever possible (for example, if asked 'what's the edible object on the table?', and there's an apple labeled apple_20 on the table, you could respond 'apple_20')
    Now, answer the following question from the drone:  Estimate the distance between the man and the drill press in centimeters

Response: i'm unable to provide an accurate distance measurement as i don't have the ability to perceive depth or distance in a 2d image. however, i can describe the scene and provide information about the objects present.
----------------------------------
