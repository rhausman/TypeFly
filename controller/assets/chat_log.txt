You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_91 x:0.64 y:0.31 width:0.22 height:0.5 color:darkslategray, chair_92 x:0.6 y:0.56 width:0.25 height:0.52 color:darkslategray, chair_93 x:0.46 y:0.41 width:0.13 height:0.37 color:darkslategray, chair_95 x:0.84 y:0.6 width:0.31 height:0.8 color:darkslategray, umbrella_98 x:0.07 y:0.29 width:0.15 height:0.42 color:darkslategray]
'task description':
[A] Move up 50cm, rotate right 30 degrees, then move forward 20cm
'response':
---
{
  "id": "chatcmpl-9AMeQ0eBTDcFliJ0TCUCYbCuGOtE6",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "mu,50;tc,30;mf,20",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1712257378,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 11,
    "prompt_tokens": 3016,
    "total_tokens": 3027
  }
}
---
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_91 x:0.72 y:0.6 width:0.35 height:0.57 color:darkslategray, chair_92 x:0.61 y:0.82 width:0.24 height:0.36 color:black, chair_95 x:0.8 y:0.83 width:0.18 height:0.34 color:black, person_110 x:0.59 y:0.35 width:0.14 height:0.41 color:rosybrown, laptop_112 x:0.97 y:0.54 width:0.05 height:0.13 color:darkslategray, laptop_104 x:0.88 y:0.64 width:0.14 height:0.15 color:dimgray, chair_114 x:0.89 y:0.89 width:0.15 height:0.22 color:black, toilet_118 x:0.07 y:0.91 width:0.14 height:0.18 color:darkslategray, mouse_115 x:0.98 y:0.76 width:0.04 height:0.03 color:mediumblue]
'task description':
[A] Move towards whoever is sitting
'response':
---
{
  "id": "chatcmpl-9AMf09TGA92i3s2BSWd6zaoWr5Oxw",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Who is sitting? Output only 'person_91', 'person_110', 'chair_92', 'chair_95', 'chair_114', 'toilet_118', 'mouse_115', 'laptop_112', or 'laptop_104'';o,_1;a",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1712257414,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 65,
    "prompt_tokens": 3106,
    "total_tokens": 3171
  }
}
---

------------- MINISPEC ERROR -------------
Original Minispec: _1=lr,'Who is sitting? Output only 'person_91', 'person_110', 'chair_92', 'chair_95', 'chair_114', 'toilet_118', 'mouse_115', 'laptop_112', or 'laptop_104'';o,_1;a
----
Error:Expected 1 arguments, but got 9.

TRACEBACK: ['  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/llm_controller.py", line 152, in execute_task_description\n    self.execute_minispec(result)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/llm_controller.py", line 134, in execute_minispec\n    return interpreter.execute(minispec)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 66, in execute\n    self.execute_function_call(statement)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 92, in execute_function_call\n    self.env[var.strip()] = self.call_function(func)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/minispec_interpreter.py", line 141, in call_function\n    return skill_instance.execute(args)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/skillset.py", line 77, in execute\n    parsed_args = self.parse_args(arg_list)\n', '  File "/Users/richardhausman/Documents/Yale/_Senior_Year/TypeFly/controller/abs/skill_item.py", line 55, in parse_args\n    raise ValueError(f"Expected {len(self.args)} arguments, but got {len(args_str_list)}.")\n']
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_91 x:0.75 y:0.58 width:0.36 height:0.57 color:darkslategray, chair_92 x:0.65 y:0.77 width:0.26 height:0.47 color:black, chair_95 x:0.87 y:0.82 width:0.24 height:0.35 color:black, person_110 x:0.27 y:0.52 width:0.24 height:0.57 color:black, tv_124 x:0.53 y:0.33 width:0.13 height:0.12 color:linen, chair_93 x:0.52 y:0.47 width:0.09 height:0.19 color:darkslategray, laptop_104 x:0.92 y:0.65 width:0.11 height:0.13 color:dimgray]
'task description':
[A] Move towards whichever person is sitting
'response':
---
{
  "id": "chatcmpl-9AMfgF4jUJcm8EFHwqrcV8u6D1knh",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which person is sitting, person_91 or person_110? Output only 'person_91' or 'person_110'';?_1/person_91{o,person_91;a}?_1/person_110{o,person_110;a}->False",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1712257456,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 58,
    "prompt_tokens": 3048,
    "total_tokens": 3106
  }
}
---

------------- LLAVA REQUEST --------
Question: Which person is sitting, person_91 or person_110? Output only 'person_91' or 'person_110

Response: person_91 </s>
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_91 x:0.57 y:0.69 width:0.56 height:0.62 color:darkslategray, laptop_104 x:0.9 y:0.88 width:0.19 height:0.25 color:darkslategray, chair_110 x:0.46 y:0.47 width:0.32 height:0.33 color:tan, chair_150 x:0.28 y:0.88 width:0.08 height:0.24 color:blue]
'task description':
[Q] describe the scene before you in detail
'response':
---
{
  "id": "chatcmpl-9AMgSmUIMqW5T0porsvvATq9wrePe",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Can you describe the scene in detail?';l,_1",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1712257504,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 17,
    "prompt_tokens": 2970,
    "total_tokens": 2987
  }
}
---

------------- LLAVA REQUEST --------
Question: Can you describe the scene in detail?

Response: in the heart of a cluttered room, a man in a vibrant red sweater is engrossed in his work on a laptop. the laptop, open and ready for use, sits on a desk that's a testament to creativity and chaos. the desk is a hive of activity, strewn with various objects that hint at the man's interests and hobbies.

to the left of the man, a computer monitor stands tall, its screen glowing with unseen information. a keyboard lies in front of it, waiting to be used. the monitor and keyboard are the only objects on the desk that seem to have a designated space.

the rest of the desk is a jumble of items. a collection of cables and wires snake across the surface, connecting various devices and creating a tangled web of technology. a pair of scissors rests on the desk, perhaps used for cutting paper or opening packages. a pair of headphones lies nearby, suggesting that the man might be listening to music or participating in an online meeting.

the room itself is a study in contrasts. a black curtain hangs in the background, providing a stark contrast to the white walls. a window punctuates one wall, offering a glimpse of the world outside.

the man appears to be in deep thought, his fingers dancing over the laptop keyboard. the scene is one of focused creativity, where every object has its place and purpose. </s>
----------------------------------
You are a drone pilot and you should follow the user's instructions to generate a MiniSpec plan to fulfill the task or give advice on user's input if it's not clear or not reasonable.
Your response should carefully consider the 'system skills description', the 'scene description', and the 'task description'.
The 'system skills description' describes the system's capabilities which include low-level and high-level skills. Low-level skills, while fixed, offer direct function calls to control the drone and acquire vision information. High-level skills, built with our language 'MiniSpec', are more flexible and can be used to build more complex skills. Whenever possible, please prioritize the use of high-level skills, invoke skills using their designated abbreviations, and ensure that 'object_name' refers to a specific type of object.
Note the special low-level skill called 'llava_request', which lets you query a multi-modal LLM which can answer nuanced questions about the scene, including texture, visual description, uncommon objects (which may not appear in the scene description), and more.
'llava_request' returns a string, so you will probably need to use its output as input to a 'query' command. Please also note that you will need to give LLaVA precise guidance of what output to produce. For example, you might consider ending your request to LLaVA with "output only 'yes' or 'no'", or "output only a number," etc.

The BNF syntax definition of 'MiniSpec' is as follows:
<program> ::= { <block-statement> [';'] | <statement> ';' }
<statement> ::= <variable-assign> | <function-call> | <return>
<block-statement> ::= <loop> | <conditional>
<loop> ::= <int> '{' <program> '}'
<function-call> ::= <function-name> { ',' <argument> }
<variable-assign> ::= <variable> '=' <function-call>
<conditional> ::= '?' <condition> '{' <program> '}'
<return> ::= '->' <value>
<condition> ::= <value> <comparator> <value> { '&' <condition> | '|' <condition> }
<comparator> ::= '>' | '<' | '==' | '!=' | '/'
<function-name> ::= <alpha> { <alpha> }
<argument> ::= <literal-value> { ',' <literal-value> }
<variable> ::= '_' <int>
<value> ::= <literal-value> | <variable> | <function-call>
<literal-value> ::= <int> | <float> | <string> | <bool>

Please note that the '/' comparator is a "has the substring" comparator. So 'a/b' would evaluate to true if and only if 'b' is a substring of a. This is especially useful when processing LLaVA responses, since it might not produce exactly the output you want. For example, if you ask LLaVA, "Is the person dressed nicely? Answer only 'yes' or 'no'", there is a chance that it could respond "yes, he is dressed fairly well". So, we would want to look for the substring "yes" rather than checking if the response is exactly equals the string "yes"

Description of the two skill sets:
- High-level skills:
abbr:s,name:sweeping,definition:8{_1=iv,$1;?_1==True{->True}tc,45}->False,args:[object_name:str],description:Rotate to find a certain object when it's *not* in current scene
abbr:sa,name:sweeping_abstract,definition:8{_1=q,$1;?_1!=False{->_1}tc,45}->False,args:[question:str],description:Rotate to find an abstract object by a description when it's *not* in current scene
abbr:a,name:approach,definition:mf,120,args:[],description:Approach a certain object
abbr:o,name:orienting,definition:4{_1=ox,$1;?_1>0.6{tc,15};?_1<0.4{tu,15};_2=ox,$1;?_2<0.6&_2>0.4{->True}}->False,args:[object_name:str],description:Rotate to align with a certain object
abbr:sl,name:sweeping_llava,definition:8{_1=lr,$1;?_1/$2{->_1}tc,45}->False,args:[question:str, substring:str],description:Rotate, consult LLaVA at each step, and check for a specific substring in its output

- Low-level skills:
abbr:mf,name:move_forward,args:[distance:int],description:Move forward by a distance
abbr:mb,name:move_backward,args:[distance:int],description:Move backward by a distance
abbr:ml,name:move_left,args:[distance:int],description:Move left by a distance
abbr:mr,name:move_right,args:[distance:int],description:Move right by a distance
abbr:mu,name:move_up,args:[distance:int],description:Move up by a distance
abbr:md,name:move_down,args:[distance:int],description:Move down by a distance
abbr:tc,name:turn_cw,args:[degrees:int],description:Rotate clockwise/right by certain degrees
abbr:tu,name:turn_ccw,args:[degrees:int],description:Rotate counterclockwise/left by certain degrees
abbr:d,name:delay,args:[milliseconds:int],description:Wait for specified microseconds
abbr:iv,name:is_visible,args:[object_name:str],description:Check the visibility of target object
abbr:ox,name:object_x,args:[object_name:str],description:Get object's X-coordinate in (0,1)
abbr:oy,name:object_y,args:[object_name:str],description:Get object's Y-coordinate in (0,1)
abbr:ow,name:object_w,args:[object_name:str],description:Get object's width in (0,1)
abbr:oh,name:object_h,args:[object_name:str],description:Get object's height in (0,1)
abbr:l,name:log,args:[text:str],description:Output text to console
abbr:p,name:picture,args:[],description:Take a picture
abbr:q,name:query,args:[question:str],description:Query the LLM for reasoning
abbr:lr,name:llava_request,args:[question:str],description:Query LLaVA for visual description, reasoning, and more


The 'scene description' is an object list of the current view, containing their names with ID, location, and size (location and size are floats between 0~1). This may not be useful if the task is about the environment outside the view.
The 'task description' is a natural language sentence, describing the user's instructions. It may start with "[A]" or "[Q]". "[A]" sentences mean you should generate an execution plan for the drone. "[Q]" sentences mean you should use 'log' to show a literal answer at the end of the plan execution. Please carefully reason about the 'task description', you should interpret it and generate a detailed multi-step plan to achieve it as much as you can while following the guides below:
Handling Typos and Ambiguous Language: When encountering typos or vague language in user inputs, strive to clarify and correct these ambiguities. If a typo is evident, adjust the input to reflect the most likely intended meaning. For vague or unclear expressions, seek additional information from the user by returning a question to the user.
Analytical Scene and Task Assessment: Evaluate the scene and task critically. If a specific condition or requirement is clearly deduced from the scene description, generate the corresponding action directly.
Relevance to Current Scene: When considering tasks, assess their relevance to the current scene. If a task does not pertain to the existing scene, disregard any details from the current scene in formulating your response.
Extraction of Key Information: Extract 'object_name' arguments or answers to questions primarily from the 'scene description'. If the necessary information is not present in the current scene, especially when the user asks the drone to move first or check somewhere else, use the query 'q,' followed by the 'question', to determine the 'object_name' or answer subsequently.

Here is a list of example 'response' for different 'scene description' and 'task description', and their explanations:
Example 1:
    scene:[],task:[A] Find a bottle and tell me it's height.
    response:_1=s,bottle;?_1==True{o,bottle;_2=oh,bottle;l,_2}
    reason:no bottle instance in the scene, so we use sweeping to find bottle, then orienting and use object_height to get the height and log to output the height
Example 2:
    scene:[apple_5],task:[A] Find an apple.
    response:o,apple;a
    reason:there is an apple instance in the scene, so we orienting and approach the apple_5
Example 3:
    scene:[apple_3],task:[Q] Is there an apple on your left?
    response:tu,90;_1=iv,apple;?_1==True{l,'Yes';->True}l,'No';->False
    reason:turn left 90 degrees, then use is_visible to check whether there is an apple on your left
Example 4:
    scene:[chair_13, laptop_2],task:[A] Go to the chair behind you.
    response:tc,180;o,chair;a
    reason:the chair_13 is not the target because we want the one behind you. So we turn 180 degrees then orienting and approach the general object chair
Example 5:
    scene:[chair_3, laptop_1, bottle_5],task:[A] Find and go any edible object.
    response:_1=sa,'what's the edible target?';?_1!=False{o,_1;a}
    reason:edible object is abstract and there is no edible object in the scene, so we use sweeping_abstract to find the edible object
Example 6:
    scene:[chair_3, laptop_9],task:[A] Turn around with 30 degrees step until you can see some animal.
    response:8{_1=q,'what's the animal target?';?_1!=False{l,_1;->True}tc,30}->False
    reason:we use loop and query to find animal
Example 7:
    scene:[chair_3, laptop_9],task:[A] If you can see a chair, go find a person, else go find an orange.
    response:_1=s,person;?_1==True{o,person;a;->True}?_1==False{_2=s,orange;?_2==True{o,orange;a;->True}}->False
    reason:From the scene, we can see a chair, so we use sweeping to find a person
Example 8:
    scene:[chair_3, laptop_9],task:[A] Go to 
    response:l,'Please give me more information.'
    reason:The task is too vague, so we use log to output the advice
Example 9:
    scene:[person_1, person_2],task:[A] Approach the angriest-looking person.
    response:_1=lr,'Which is angrier, the person_1 or person_2? Output only 'person_1' or 'person_2'';?_1/person_1{o,person_1;a}?_1/person_2{o,person_2;a}->False
    reason:We use lr to ask llava which person is angrier. Then, depending on the output, we orient towards and approach the appropriate person.
Example 10:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[A] Find and approach a trampoline in the area.
    response:_1=sl,'Does the frame contain a trampoline? Answer only 'yes' or 'no'','yes';?_1!=False{a;->True}->False
    reason:Trampolines are unusual objects, so we use sweeping_llava to check the surroundings until we find one, and once we do, we approach. If we complete the sweep without encountering one, we do nothing.
Example 11:
    scene:[tree_1, tree_2, tree_3, tree_4, tree_5],task:[Q] What's the color of the beanbag in the area, if any?
    response:_1=sl,'Does the frame contain a beanbag? Answer only 'yes' or 'no'','yes';?_1!=False{_2=lr,'What is the color of the beanbag in the frame?';l,_2;->True}l,'There is no beanbag in the area';->False
    reason:We use sweeping_llava to check the area for beanbags. If one is found, we can then ask llava what the color of the beanbag is, and log the output. If we fail to find one, we return False.

Here are potential plans and error messages from your previous responses, if any (otherwise they will be empty), please read them carefully and try to generate a better response:
None

Now given the following 'scene description' and 'task description', please generate the response only with a single sentence of MiniSpec program.
'scene description':
[person_91 x:0.56 y:0.69 width:0.59 height:0.63 color:darkslategray, chair_150 x:0.27 y:0.89 width:0.07 height:0.22 color:mediumblue, chair_127 x:0.27 y:0.61 width:0.14 height:0.32 color:black, laptop_104 x:0.92 y:0.88 width:0.17 height:0.24 color:darkslategray, person_110 x:0.57 y:0.41 width:0.27 height:0.41 color:darkslategray, tv_151 x:0.3 y:0.36 width:0.19 height:0.16 color:midnightblue]
'task description':
[Q] describe only the person looking at you
'response':
---
{
  "id": "chatcmpl-9AMgrgHE49hUkOJIXAaCkmi2pebV5",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "_1=lr,'Which person is looking at me? Output only 'person_91' or 'person_110'';_2=lr,'Describe ' + _1;l,_2",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null
      }
    }
  ],
  "created": 1712257529,
  "model": "gpt-4-0613",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 39,
    "prompt_tokens": 3029,
    "total_tokens": 3068
  }
}
---

------------- LLAVA REQUEST --------
Question: Which person is looking at me? Output only 'person_91' or 'person_110

Response: 'person_91' </s>
----------------------------------

------------- LLAVA REQUEST --------
Question: Describe ' + _1

Response: in the image, there are two individuals in a room that appears to be a workspace or a garage. the person in the foreground is seated at a desk with a laptop open in front of him. he is wearing a red sweater and is looking directly at the camera. behind him, there is another person who is bending over a table, possibly working on something. the room is cluttered with various items, including electronics, boxes, and other miscellaneous objects, suggesting a busy and active environment. the lighting in the room is bright, and there are windows letting in natural light. the overall atmosphere seems to be one of focused activity. </s>
----------------------------------
